<img src="/static/dataiku/images/dss-logo-about.png" width="70" style="float: right; margin-right: 30px" />
<br></br>

# Overview
This project takes PDF files as input, uses Optical Character Recognition (OCR) to extract the text for each input PDF, classifies the document into one of twenty article categories and summarises the text to an arbitrary number of sententences (by default 3 using Latent Semantic Analysis (LSA)), before composing the document title, classification, summary and original text into a text file which is emailed to the user.

# Data
Arbitrary PDF files can be placed into the [input folder](managed_folder:QZb3pfTL) for classfication and summarisation.

The document classification model was trained on the 20 newsgroups dataset ([documentation](http://qwone.com/~jason/20Newsgroups/)) which is publicly available from sci-kit learn as described [here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#).

# Project Components

## OCR for Text Extraction

The PDFs are first converted to image files using pdf2image ([documentation](https://pypi.org/project/pdf2image/)) in this [python recipe](recipe:compute_HNEvJqgm). The OCR is then performed using Google's open-source Tesseract library ([documentation](https://github.com/tesseract-ocr/tesseract#tesseract-ocr)).

